{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d608126-f0c1-47eb-90bf-2232f49e7f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Generic Functions\n",
    "print(\"instantiating spark session\")\n",
    "from pyspark.sql.session import *\n",
    "def return_sparksession(appname):\n",
    "    return SparkSession.builder.appName(appname).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b60f659-05e8-44c0-a02d-4797dae834f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"defining read function\")\n",
    "def return_df(sparksess,path,form='csv',delimiter=',',head=True,infsch=False,mulline=False):\n",
    "    if form=='csv':\n",
    "        df=sparksess.read.format(form).option(\"delimiter\",delimiter).option(\"inferSchema\",infsch).option(\"header\",head).load(path)\n",
    "    elif form=='json':\n",
    "        df=sparksess.read.format(form).option(\"multiLine\",mulline).load(path)\n",
    "    elif form=='delta':\n",
    "        df=sparksess.read.format(form).load(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352c61db-e426-46a9-a236-bed638422ddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"defining writing function\")\n",
    "def write_df(df,tgt,tgtformat=None,mo='overwrite'):\n",
    "    if tgtformat == 'csv':\n",
    "        df.write.mode(mo).format(tgtformat).option(\"delimiter\",\",\").option(\"header\",True).save(tgt)\n",
    "    elif tgtformat == 'json':\n",
    "        df.write.mode(mo).format(tgtformat).save(tgt)\n",
    "    elif tgtformat == 'delta':\n",
    "        df.write.mode(mo).format(tgtformat).save(tgt)\n",
    "    elif tgtformat=='tbl':\n",
    "        df.write.mode(mo).saveAsTable(tgt)\n",
    "    else:\n",
    "        print(\"Invalid target format -  for now our fw supports csv,json,delta,table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7c9a1a-abfd-4dcf-81a6-7a3601dfcecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ETL generic functions\n",
    "def staff_munging(df,opt,cols):\n",
    "    return df.na.drop(how=opt,subset=cols)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generic_functions_nb1.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
