{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8153fe1a-ab7e-4b91-9e7a-1923a1d52e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#By Knowing this notebook, we can become an eligible \"Data Egress Developer/Engineer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a060fac0-8caf-4ad2-bafb-5e307f2222fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1 = spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\", header=True, sep=\",\",inferSchema=True,samplingRatio=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7e8f1f-ac72-4ef1-9054-41f9d06470b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Writing the data in Builtin - different file formats & different targets (all targets in this world we can write the data also...)  \n",
    "####1. Writing in csv (structured data (2D data Table/Frames with rows and columns)) format with few basic options listed below (Schema (structure) Migration)  \n",
    "custid,fname,lname,age,profession -> custid~fname~lname~prof~age\n",
    "\n",
    "- header\n",
    "- sep\n",
    "- mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcfb7b2-20bc-4acf-afe4-e8c5a5d73d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are performing schema migration from comma to tilde delimiter\n",
    "ingest_df1.write.csv(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout\", header=True, sep=\"~\", mode=\"overwrite\")\n",
    "#4 modes of writing - append,overwrite,ignore,error\n",
    "display(ingest_df1.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6723efde-4b83-4bc8-92d4-7c88ac055b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are performing schema migration by applying some transformations (this is our bread and butter that we learn exclusively further)\n",
    "transformed_df = ingest_df1.select(\"custid\",\"fname\",\"lname\",\"profession\",\"age\").withColumnRenamed(\"profession\",\"prof\")#DSL transformation (not for now...)\n",
    "transformed_df.write.csv(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout\", header=True, sep=\"~\", mode=\"overwrite\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c66540c-f252-4271-a583-1cb2131385b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Writing in json format with few basic options listed below\n",
    "path  \n",
    "mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b22c019-15f1-448e-b7ed-ca12076201d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- We did a schema migration and data conversion from csv to json format (ie structued to semi structured format)\n",
    "- json - we learn a lot subsequently (nested/hierarchical/complex/multiline...),\n",
    "- what is json - fundamentally it is a dictionary of dictionaries\n",
    "- json - java script object notation\n",
    "- Standard json format (can't be changed) - {\"k1\":\"string value\",\"k2\":numbervalue,\"k3\":v2} where key has to be unique & enclosed in double quotes and value can be anything\n",
    "- when to go with json or benifits -\n",
    "a. If we have data in a semistructure format (with variable data format with dynamic schema)  \n",
    "eg. {\"custid\":4000001,\"profession\":\"Pilot\",\"age\":55,\"city\":\"NY\"}\n",
    "{\"custid\":4000001,\"fname\":\"Kristina\",\"lname\":\"Chung\",\"prof\":\"Pilot\",\"age\":\"55\"}  \n",
    "b. columns/column names or the types or the order can be different  \n",
    "c. json will be provided by the sources if the data is dynamic in nature (not sure about number or order of columns) or if the data is api response in nature.  \n",
    "d. json is a efficient data format (serialized/encoded) for performing data exchange between applications via network & good for parsing also & good for object by object operations (row by row operation in realtime fashion eg. amazon click stream operations)  \n",
    "e. json can be used to group or create hierarchy of data in a complex or in a nested format eg. https://randomuser.me/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0ca9f2-82b9-494e-87d0-6de9645e3e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.json(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout/jsonout\",mode='append')\n",
    "#custid,fname,lname,age,profession -> {\"custid\":4000001,\"fname\":\"Kristina\",\"lname\":\"Chung\",\"prof\":\"Pilot\",\"age\":55}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da58d78-8baa-44c0-89ef-2e70a824dab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3.Serialization & Deserialization File formats (Binary/Brainy File formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e55910c-b07a-4064-b563-dd8d1207fcdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Data mechanics\n",
    "1. encoding/decoding(machine format) - converting the data from human readable format to machine understandable format for performant data transfer (eg. Network transfer of data will be encoded)\n",
    "2. *compression/uncompression(encoding+space+time) - shrinking the data in some format using some libraries (tradeoff between time and size) (eg. Compress before store or transfer) - snappy is a good compression tech used in bigdata platform\n",
    "3. encryption (encoding+security) - Addition to encoding, encryption add security hence data is (performant+secured) (using some algos - SHA/MD5/AES/DES/RSA/DSA..)\n",
    "4. *Serialization (applicable more for bigdata) - Serialization is encoding + performant by saving space + processing intelligent bigdata format - Fast, Compact, Interoperable, Extensible (additional configs), Scalable (cluster compute operations), Secured (binary format)..\n",
    "5. *masking - Encoding of data (in some other format not supposed to be machine format) which should not be allowed to decode (used for security purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbe2fd0-eca5-44dd-b95b-76ce7bf880f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are the (builtin) serialized file formats we are going to learn? orc parquet delta(databricks properatory)\n",
    "\n",
    "- We did a schema migration and data conversion from csv/json to serialized data format (ie s  tructued to sturctured(internall binary unstructured) format)  \n",
    "- We learn/use a lot/heavily subsequently,  \n",
    "- what is serialized - fundamentally they are intelligent/encoded/serialized/binary data formats applied with lot of optimization & space reduction strategies..  \n",
    "- orc - optimized row column format  \n",
    "- parquet - tiled data format  \n",
    "- delta(databricks properatory) enriched parquet format - Delta (modified) operations can be performed  \n",
    "- format - serialized/encoded , we can't see with mere eyes, only some library is used deserialized/decoded data can be accessed as structured data  \n",
    "- **when to go with serialized or benifits** -  \n",
    "a. For storage benifits for eg. orc will save 65+% of space for eg. if i store 1gb data it occupy 350 space, with compression it can improved more...  \n",
    "b. For processing optimization. Orc/parquet/delta will provide the required data alone if you query using Pushdown optimization .  \n",
    "c. Interoperability feature - this data format can be understandable in multiple environments for eg. bigquery can parse this data.  \n",
    "d. Secured\n",
    "**In the projects/environments when to use what fileformats - we learn in detail later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e54dd9c-eb50-428e-970b-fcf6657d00f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.orc(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcout\",mode='overwrite',compression='zlib')#by default orc/parquet uses snappy compression\n",
    "spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcout\").show(2)#uncompression + deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d948e1-6e71-4c02-82a3-6786c4307088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Orc/Parquet follows WORM feature (Write Once Read Many)\n",
    "ingest_df1.write.mode(\"overwrite\").option(\"compression\",\"gzip\").option(\"compression\",\"snappy\").parquet(path=\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetout\")#by default orc/parquet uses snappy compression\n",
    "spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetout\").show(2)#uncompression + deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d50d36-6332-472a-9389-2795dbcf6cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Delta follows WMRM feature (Write Many Read Many) - We did Delta Lake creation (Datalake + Delta file format)\n",
    "ingest_df1.write.format(\"delta\").save(\"/Volumes/workspace/wd36schema/ingestion_volume/target/deltaout\",mode='overwrite')\n",
    "spark.read.format(\"delta\").load(\"/Volumes/workspace/wd36schema/ingestion_volume/target/deltaout\").show(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13452fc1-5c38-4d1c-b95d-6f25a7ba922d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.Table Load Operations - Building LAKEHOUSE ON TOP OF DATALAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7555e4-415e-4a00-96c1-444810d88651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Can we do SQL operations directly on the tables like a database or datawarehouse? or Can we build a Lakehouse in Databricks?\n",
    "\n",
    "- We learn/use a lot/heavily subsequently,  \n",
    "- what is Lakehouse - A SQL/Datawarehouse/Query layer on top of the Datalake is called Lakehouse  \n",
    "- We have different lakehouses which we are going to learn further -  \n",
    "1. delta tables (lakehouse) in databricks  \n",
    "2. hive in onprem  \n",
    "3. bigquery in GCP  \n",
    "4. synapse in azure  \n",
    "5. athena in aws  \n",
    "- when to go with lakehouse  \n",
    "a. Transformation  \n",
    "b. Analysis/Analytics  \n",
    "c. AI/BI  \n",
    "d. Literally we are going to learn SQL & Advanced SQL  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef89651b-a71f-4ccd-9701-3e87b26680dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "altering the table in below cell as previously created is having age column as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0a5fff-9a52-4391-a54e-ebdf4ecfe1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE workspace.wd36schema.lh_custtbl\n",
    "CHANGE COLUMN age age INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae3b4a21-c481-4de5-b3d5-a4459a951a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Add a new column with INT type\n",
    "ALTER TABLE workspace.wd36schema.lh_custtbl\n",
    "ADD COLUMN age_int INT;\n",
    "\n",
    "-- Update the new column with converted values\n",
    "UPDATE workspace.wd36schema.lh_custtbl\n",
    "SET age_int = CAST(age AS INT);\n",
    "\n",
    "-- Drop the old column\n",
    "ALTER TABLE workspace.wd36schema.lh_custtbl\n",
    "DROP COLUMN age;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b83ba3-1e10-46e5-9416-3aa2b6dcdd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We are building delta tables in databricks (we are building hive tables in onprem/we are building bq tables in gcp...)\n",
    "#saveastable (named notation/named arguments)\n",
    "#Table\n",
    "#cid,prof,age,fname,lname\n",
    "#mapping\n",
    "#cid,prof,age,fname,lname\n",
    "ingest_df1.write.saveAsTable(\"workspace.wd36schema.lh_custtbl\",mode='overwrite')\n",
    "#display(spark.sql(\"show create table workspace.wd36schema.lh_custtbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffa0e675-d7f9-4f37-967f-ee02c1bd3303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. insertinto function can be used as like saveAstable with few differences\n",
    "#a. it works only if the target table exist\n",
    "#b. it works by creating insert statements in the behind(not bulk load), hence it is slow, hence we have use for small dataset (safely only if table exists)\n",
    "#c. it will load the data from the dataframe by using position, not by using name..\n",
    "#insertInto (positional notation/positional arguments)\n",
    "#Table\n",
    "#cid,prof,age,fname,lname\n",
    "#mapping.\n",
    "#cid,fname,lname,age,prof\n",
    "ingest_df1.write.insertInto(\"workspace.wd36schema.lh_custtbl\",overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57cea775-6082-4030-a33c-7dda7159214c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.format(\"delta\").save(\"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55d78b67-5204-4a05-933b-b6529090d9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I am using spark engine to pull the data from the lakehouse table backed by dbfs (s3) (datalake) where data in delta format(deltalake) \n",
    "display(spark.sql(\"select * from workspace.wd36schema.lh_custtbl\"))#sparkengine+lakehouse+datalake(deltalake)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0203ce-5479-435d-b070-f1d16bdc33f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####\n",
    "5. XML Format - Semi structured data format (most of the json features can be applied in xml also, but in DE world not so famous like json)  \n",
    "- Used rarely on demand (by certain target/source systems eg. mainframes)  \n",
    "- Can be related with json, but not so much efficient like json  \n",
    "- Databricks provides xml as a inbuild function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b833c97-1e25-417b-a78f-8e46917b63ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Modes in Writing  \n",
    "1.**Append **- Adds the new data to the existing data. It does not overwrite anything.  \n",
    "2.**Overwrite ** - Replaces the existing data entirely at the destination.  \n",
    "3. **ErrorIfexist**(default) - Throws an error   if data already exists at the destination.  \n",
    "4. **Ignore** - Skips the write operation if data already exists at the destination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06e87e1-7d04-4de9-ba9e-77de0ac6fe7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingest_df1.write.xml(\"/Volumes/workspace/wd36schema/ingestion_volume/target/xmlout\",mode=\"ignore\",rowTag=\"cust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fefba40-810b-4411-a58b-c223cdf668a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark dataframe write operations in different formats and targets?\n",
    "\n",
    "1. df.write.csv/json/orc/parquet/table/xml... operations & df.write.format('delta').save()\n",
    "2. Few of the important read options under csv such as header, sep, mode(append/overwrite/error/ignore), toDF.\n",
    "3. Few additional options such as compression, different file formats...\n",
    "4. Few of the important read options under csv such as header, sep, mode(append/overwrite/error/ignore), toDF.\n",
    "5. Few additional options such as compression, different file formats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fdb721b-c656-432b-b651-bf4d95bd8f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4718454501993997,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3-Basic-WriteOps.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
