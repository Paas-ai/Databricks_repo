{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31194c7-3c6e-46da-abfb-0e9a2d41f64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####First Let's understand the basic Catalog + Volume Feature of Databricks\n",
    "Filesystem Hierarchy of Volume in Databricks (DBFS)?  \n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/volume/folder/data files  \n",
    "Tables Hierarchy of Databricks?  \n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/tables/data(dbfs filesystem/some other filesystems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de92582-8e8d-4790-b8c3-3f5d8918d27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists catalog1_dropme;\n",
    "create database if not exists catalog1_dropme.schema1_dropme;\n",
    "create volume if not exists catalog1_dropme.schema1_dropme.volume1_dropme;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e92aff-9117-4c31-9dbd-ddb0e7018a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab0d1aa-5ae1-4534-9345-f3cce70f91ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we need to create schema/volume/folder programatically, follow the below steps  \n",
    "## Spark SQL  \n",
    "###1.E(Extract)\n",
    "L(Load)  \n",
    "Inbuilt libraries sources/targets & Inbuilt data Formats  \n",
    "2. Bread & Butter (T(Transformation) A(Analytical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6fdafbb-9375-4044-84d2-4a196ceb8970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/workspace/wd36schema2/volume1/folder1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57102233-458c-4211-8fcd-a3f007cb621c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Learn How to Create Dataframes from filesystem using different options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903f6c8a-93db-483e-807e-8b5a4e8b6401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below drive url  \n",
    "[https://drive.google.com/drive/folders/1Tw7V9eBtUxy0xQMW38z3-bzWI_ewzLm6?usp=drive_link](url)\n",
    "\n",
    "#### How Do We Write a Typical Spark Application (Core(Obsolete),SQL(Important),Streaming(Mid level important)) \n",
    "Before we Create Dataframe/RDD, what is the prerequisite? We need to create spark session object by instantiating sparksession class (by default databricks did that if you create a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471e97c8-d481-417d-bd7f-624109bf9ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark) #already instantiated by databricks\n",
    "spark1 = SparkSession.builder.getOrCreate()\n",
    "print(spark1) #we instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bee484-1992-4d4c-8cfa-37ffe562bca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a DBFS volume namely commondata and upload the above data in that volume What are other FS uri's available? file:///, hdfs:///, dbfs:///, gs:///, s3:///, adls:///, blob:///"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed8ebc2-7846-45de-a660-c0f1e7baeeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin formats (csv/json/orc/parquet/delta/tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6959ae89-ddbf-4595-95a4-e1028b3a87d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If I don't use any options in this csv function, what is the default functionality?\n",
    "#1. By default it will consider ',' as a delimiter (sep='~')\n",
    "#2. By default it will use _c0,_c1..._cn it will apply as column headers (header=True or toDF(\"\",\"\",\"\") or we have more options to see further)\n",
    "#3. By default it will treat all columns as string (inferSchema=True or we have more options to see further)\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f453c56-d9b3-42ee-aa35-e94a46bed9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Header Concepts (Either we have define the column names or we have to use the column names from the data)\n",
    "#Important option is...\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, but we are asking spark to take the first row as header and not as a data?\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\",header=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "#csv_df1.write.csv(\"/Volumes/workspace/wd36schema2/volume1/folder1/outputdata\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "csv_df2=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9776c47b-c580-4c80-b595-85a22a0c4b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "csv_df1.printSchema()\n",
    "csv_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c44038-9263-4725-b055-1dbc2fa14252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n",
    "#sample data\n",
    "#4004979,Tara,Drake,32,\n",
    "#4004980,Earl,Hahn,34,Human resources assistant\n",
    "#4004981,Don,Jones,THIRTY SIX,Lawyer\n",
    "#csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_2.txt\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_2\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df1.where(\"id in (4000001,4000002)\").show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0751623f-fd33-426e-ae86-fcd63656ee38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Using delimiter or seperator option\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\",header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e1c8c4-18a3-4988-a648-7d2bdb1b67bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies with 3 ways of creating dataframes)\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df2=spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"true\").option(\"sep\",\"~\").format(\"csv\").load(\"dbfs:/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df2.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\")\n",
    "csv_df3.show(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9da5de-38a3-440f-a367-6aece706c655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57990999-f7ef-43b6-a87a-e2ba795807bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header_oth_del\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66d1d71-13ad-496b-b09c-57dca476ebde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generic way of read and load data into dataframe using extended options from external sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d332d7-b924-4c8d-8401-ae75fb66b1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e41a6328-de01-4c9f-ba42-4e15073c907a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading data from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1f0b8f-918e-4f0a-a412-b69e409acaf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",inferSchema=True,header=True,sep='~')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85ef54c-9369-4313-92d0-e2312f24b7e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_df1=spark.read.csv(path=[\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd1be46f-0b41-4012-bd4b-0d43c5877c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Requirement: I am getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f362c70e-8a5b-4b8e-a555-ccded5888e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/NY\",\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/TX\"],inferSchema=True,header=True,sep=',',pathGlobFilter=\"custs_header_*\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2960f10-7f40-4db0-8ce9-b7ef254d0e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Provide schema with SQL String or programatically (very very important)  \n",
    "PySpark SQL Datatypes  \n",
    "Data Types  \n",
    "  \n",
    "To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF  \n",
    "We are going to learn additionally 2 more options to handle schema (colname & datatype)?  \n",
    "1. Using simple string format of define schema.  \n",
    "IMPORTANT: 2. Using structure type to define schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10428816-6d93-4a3f-a0d7-489aeca4b8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bba299-32e3-4ed5-b9bc-8ac1f7e41a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Using simple string format of define custom simple schema.\n",
    "str_struct = \"id integer, fname string, lname string, age string, prof string\"\n",
    "csv_df1=spark.read.schema(str_struct).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2bba5b8-58fc-485e-ac4c-42ab80932cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Important part - /using structure type to define custom complex schema.\n",
    "##4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure = StructType([\n",
    "#   StructField(\"colname\", DataType(), True),\n",
    "#   StructField(\"colname\", DataType(), True)...])\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "custom_schema = StructType([StructField(\"id\", IntegerType(), False), StructField(\"fname\", StringType(), True), StructField(\"lname\", StringType(), True), StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_header\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cb9e83e-3f0a-4565-b0b4-7e7126900f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Reading data from other formats (Try the below usecases after completing the 3-Basic-WriteOps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cd833a8-0212-4f06-b35a-77259f00891b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"~\").csv(\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout/\")\n",
    "df1.show(2)\n",
    "display(df1.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1701fb37-d32c-4754-8e4f-267eca23e5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Reading json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92f7109-74ea-4ab6-81a5-bdb32f2d72d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dfjson1=spark.read.json(\"/Volumes/workspace/wd36schema/ingestion_volume/target/jsonout\")\n",
    "dfjson1.show(2)\n",
    "display(dfjson1.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab4eaf1e-8c27-4895-8c85-2097e8cbcf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3.Reading xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c95d8fa-976a-4107-af8b-66a094b5455a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfxml1=spark.read.xml(\"/Volumes/workspace/wd36schema/ingestion_volume/target/xmlout\",rowTag=\"cust\")\n",
    "dfxml1.show(2)\n",
    "display(dfxml1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efd09b9f-2e0e-49ba-8352-3be2d5985486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Reading Serialized data (orc/parquet/delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b60fee-fe85-470b-b438-c8d14ffdbb2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"orcdata\")\n",
    "dforc1=spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcout\")\n",
    "dforc1.show(2)\n",
    "display(dforc1.take(2))\n",
    "print(\"parquetdata\")\n",
    "dfparquet1=spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetout\")\n",
    "dfparquet1.show(2)\n",
    "display(dfparquet1.take(2))\n",
    "print(\"deltadata\")\n",
    "dfdelta1=spark.read.format(\"delta\").load(\"/Volumes/workspace/wd36schema/ingestion_volume/target/deltaout\")\n",
    "dfdelta1.show(2)\n",
    "display(dfdelta1.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "755ec07b-05b2-4a16-bd86-b2e873c030cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Reading delta (hive) table data (Lakehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da9d314-816f-406f-813e-3b643e06ed68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dftable1=spark.read.table(\"workspace.wd36schema.lh_usgtbl\")\n",
    "dftable1.show(2)\n",
    "display(dftable1.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca88609c-42ec-4990-adfc-5d6f574fb054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark csv read operations?\n",
    "\n",
    "1. How to create/manage Catalog & Volume\n",
    "2. spark session\n",
    "3. How to create some sample data and push it to the volume/folder/file\n",
    "4. spark.read.csv operations & spark.read.format().load()\n",
    "5. Few of the important read options under csv such as header, sep, inferSchema, toDF.\n",
    "6. How to define custom schema/structure using mere string with colname & datatype or by using StructType([StructField(colname,DataType()...)]) (very important).\n",
    "7. Few additional options such as samplingRatio, recursiveFileLookup & pathGlobFilter for accessing folders and subfolders with some pattern of filenames\n",
    "8. How to Read data from different sources and different file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e43376c-2a95-4ae5-a7bf-146fc28eac74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6170252700286899,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Basic-Readops.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
