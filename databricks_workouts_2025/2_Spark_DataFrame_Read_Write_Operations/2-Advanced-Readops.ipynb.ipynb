{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05201067-031e-4c77-952a-6bc6ad8e6dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. CSV Advanced Features -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b61146-72cc-48c2-98c5-c64620b91e79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Very Important - path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None,header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None,  \n",
    "Important - mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None,**\n",
    "Not Important but good to know once - encoding: Optional[str]=None, comment: Optional[str]=None,ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00660c9a-8df0-4890-b4df-9dc29c0c7b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A. Options for handling quotes & Escape\n",
    "id,name,remarks 1,'Ramesh, K.P','Good performer' 2,'Manoj','Needs ~'special~' attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b86d7d-367e-42d0-aa1f-a90560d89961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=\",\",mode=\"permissive\",comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\",escape=\"|\")\n",
    "df1.show(10,False)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "976197c3-9846-48de-8fe0-f05106acc2fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \n",
    "B. Comments, Multi line, leading and trailing whitespace handling, null and nan handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e9f277-ad00-449b-8d64-8ef2ca58d204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "struct1=\"custid int, name string, height float, jiondt date,age string\"\n",
    "df2=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata2.txt\",header=False,mode=\"permissive\", multiLine=True, quote=\"'\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,nullValue='na', nanValue=-1,maxCharsPerColumn='100',modifiedAfter='2025-12-19',dateFormat = \"yyyy-dd-MM\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bee779b-1875-4541-b508-d7e3829c9f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**C. Read modes in csv (Important feature)**  \n",
    "If any data challenges (malformed data) such as format issue/column numbers (lesser/more than expected) issue etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45565b08-1cbc-4234-8c3f-7f3bed375d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### There are 3 typical read modes and the default read mode is permissive  \n",
    "1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "2. dropMalformed — Drops all rows containing corrupt records.\n",
    "3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b096dea1-4a41-4c97-8251-8b9c633b2b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We learned about few important features mode, columnNameOfCorruptRecord, Quote, Comment\n",
    "#Question - Corrupt_record column consume more memory because it capturing all the column values(incorrect) in one column. ? Useful for doing RCA (Root Cause Analysis/Debugging)\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=\",\",mode=\"permissive\",comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=\",\",mode=\"dropMalformed\",comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=\",\",mode=\"failFast\",comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "#df1.filter(\"corrupt_record is not null\").write.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\")\n",
    "#spark.read.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7073ecc5-5d02-4e0a-8acb-e19cb6173255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### JSON Advanced features -\n",
    "**Very Important**  path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup  \n",
    "No header, No inferSchema, No sep in json...  \n",
    "**Important** - primitivesAsString(don't do inferSchema), prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory)  \n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27918ed5-8743-46ac-a0fb-9c6442d21b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option\n",
    "#primitivesAsString = inferSchema=False\n",
    "dfjson1=spark.read.json(\"/Volumes/workspace/default/volumewd36/\",primitivesAsString=True)\n",
    "dfjson1.printSchema()\n",
    "#prefersDecimal\n",
    "dfjson1=spark.read.json(\"/Volumes/workspace/default/volumewd36/\",prefersDecimal=True) #to get only decimal values\n",
    "dfjson1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b243b5f-c588-476c-b0e2-d73401e3ffd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If we don't define structure, it will infer the schema\n",
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/\",samplingRatio=1,allowUnquotedFieldNames=True,allowSingleQuotes=True,modifiedAfter='2025-12-22',dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",pathGlobFilter=\"simple_json.tx*\",recursiveFileLookup=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a24f0-efc0-479d-ba27-4a680d371768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/\",allowUnquotedFieldNames=True,allowSingleQuotes=True,modifiedAfter='2025-12-22',dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",pathGlobFilter=\"simple_json2.tx*\",recursiveFileLookup=True,\n",
    "                                     allowComments=True,lineSep='~',useUnsafeRow=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847cd000-a438-4a69-be56-0ae1d3d47a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/simple_json_multiline3.txt\",dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",\n",
    "                                     allowComments=True,multiLine=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ce78c9-c1e1-4dd7-b29f-a0e648b2c6a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Serialized data Advanced Feature - orc, parquet/delta (very very important & we learn indepth)\n",
    "- PathOrPaths\n",
    "**mergeSchema** - Important inteview property (make it proactive/ make it driven in the interview) SCHEMA EVOLUTION\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter - Problem statement : Source is sending data in any way they want ... Day1/source1-5cols, Day2/source2 - 7 cols\n",
    "\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cc44518-d4e5-4065-90c6-9e24a6c1fff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Case study: If the source (external) is sending data to us, if our consumer (Datascience/Dataanalytics) is directly communicated with our source system and asked them to propogate more/less attributes/features without the knowledge of the Dataengineering team? Here we have implement the strategy of Schema Evolution using option mergeSchema\n",
    "#Story building for interview: We get product data from source which get evolved on a frequent basis for eg. product data originally sent without gendra, costprice, purchaseprice, profit/loss metrics, demant information...\n",
    "#Steps to follow:\n",
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af78229-0b2f-43af-b8a8-57dca982a32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "day1df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source1.txt\",header=True,inferSchema=True)\n",
    "day1df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')\n",
    "day2df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source2.txt\",header=True,inferSchema=True)\n",
    "day2df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')\n",
    "day3df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source3.txt\",header=True,inferSchema=True)\n",
    "day3df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d686c6a1-b0f1-4046-8b1e-c299f02fccb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created...\n",
    "post_day3=spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput/\",mergeSchema=True)\n",
    "display(post_day3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec242469-c97b-4fff-920d-693c502e1690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "#Here we use inferSchema without an option and We can't use structuretype, because schema is evolving...\n",
    "day1df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source1.txt\",header=True,inferSchema=True)\n",
    "day1df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')\n",
    "day2df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source2.txt\",header=True,inferSchema=True)\n",
    "day2df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')\n",
    "day3df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/source3.txt\",header=True,inferSchema=True)\n",
    "day3df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d3d6d3-c855-491a-a145-4340dfa1caaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created...\n",
    "post_day3=spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput/\",mergeSchema=True)\n",
    "display(post_day3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd1f009-f2b2-47f1-b7af-65cb110ac546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4. Reading data from other formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "897f03b3-6356-4797-8bf2-1c5c5559b46d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Reading csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435b6647-34db-47c0-9ab6-45644a027e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dfdfe09-ed94-4d74-95a4-3db1827f36a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Reading JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0955c1-7c22-461b-8443-134c641f88c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json(\"/Volumes/workspace/wd36schema/ingestion_volume/target/jsonout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a890a45b-9146-48c6-b701-82eeea3b7094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Reading xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16147ff-fc89-4748-9904-50c52c06b15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.xml(\"/Volumes/workspace/wd36schema/ingestion_volume/target/xmlout\",rowTag=\"cust\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd672b10-4dac-4d44-8b16-e13d1d53e073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Reading serialized data (orc/parquet/delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfd25b4-35da-49bc-a366-607ffc2a983e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcout\").show(2)\n",
    "spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetout\").show(2)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/workspace/wd36schema/ingestion_volume/target/deltaout\").show(2)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f035d672-d875-42b4-bb0f-85ca63c05637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Reading delta/hive table data - We will heavily learn this under Databricks (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3499d79-6734-4271-96f0-5be43d1ca9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Advanced-Readops.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
