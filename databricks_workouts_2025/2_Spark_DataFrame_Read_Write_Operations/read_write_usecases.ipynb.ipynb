{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed8f94a-628d-4801-82c4-b38e4c7020dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Telecom Domain ReadOps & Write Ops Assignment - Building Datalake & Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5770fba3-468a-494c-8190-c00344779e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook contains assignments to practice Spark read options and Databricks volumes.  \n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3397407f-7bd1-4948-af6e-825c8ae66376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "713ea2ac-36b3-4a90-a353-e0e9db58ce96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97ce2e3-a351-425c-996f-467530a8b68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Write SQL statements to create:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123f3613-0dde-4846-9b5c-85d71a030744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c848a569-452b-4653-963f-10d794448f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create database if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c48150a0-bbd9-4498-ba0d-57b446dcaab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4.Using dbutils.fs.mkdirs, create folders:\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/ /Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/ /Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc4ff84-3622-4b60-87f8-0c1210024bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07707a48-4965-4785-b589-82657804d64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):  \n",
    "a. Volume vs DBFS/FileStore  \n",
    "b. Why production teams prefer Volumes for regulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fecebc9-4754-4656-b6e3-cdc1e019127c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Volumes:\n",
    "- Managed by Unity Catalog, providing centralized access control, auditing, and organization.\n",
    "- Primarily intended for non-tabular data (e.g., images, PDFs, libraries, unstructured files).\n",
    "- Accessed using the secure, structured path /Volumes/<catalog>/<schema>/<volume>/<path>.\n",
    "- Seamlessly integrates with the Unity Catalog data hierarchy alongside tables and schemas. \n",
    "\n",
    "DBFS file storage:\n",
    "- Uses legacy access patterns (e.g., IAM roles, storage keys configured on the cluster) with limited governance  \n",
    "- Can store any data\n",
    "- Accessed using paths like /dbfs/... or legacy mount points (/mnt/...), which are less secure.\n",
    "- A separate abstraction layer over cloud storage that uses different, less secure access models.\n",
    "\n",
    "####Why Production teams are prefereing Volumes for regulated data?  \n",
    "Production teams prefer Volumes for regulated data due to the robust, centralized governance capabilities provided by the Unity Catalog, which are essential for meeting stringent security and compliance standards like HIPAA and PCI-DSS  \n",
    "Key reasons for this preference include:  \n",
    "**Centralized and Fine-Grained Access Control**: Volumes allow administrators to define access policies using standard ANSI SQL at a granular level (catalog, schema, volume, or even file level), ensuring that only authorized users or groups can access sensitive non-tabular data. In contrast, DBFS mounts often provide broad access across the workspace, which is a security concern for regulated data.  \n",
    "**Comprehensive Auditing and Lineage**: Unity Catalog automatically captures user-level audit logs and end-to-end data lineage, which is crucial for compliance verification and audit readiness. This provides transparency into how data moves and is consumed, a feature largely absent in the legacy DBFS.  \n",
    "**Compliance-by-Design Environment**: Databricks has specific features, like the Compliance Security Profile, designed to help organizations meet industry standards such as HIPAA, PCI-DSS, and FedRAMP. Using volumes within this framework helps enforce enhanced security settings across the storage layer.  \n",
    "**Abstraction of Cloud Credentials**: With managed volumes, production teams don't need to manually manage complex cloud credentials or storage paths. This simplifies data management and reduces the risk of credential exposure. \n",
    "**Isolation and Secure Sharing**: Volumes allow for better data isolation. Catalogs can be bound to specific workspaces, ensuring that production data is processed only in designated, secure environments, and sensitive data can be securely shared with external partners using Delta Sharing without data duplication.  \n",
    "**Deprecation of DBFS**: Databricks strongly recommends against using the DBFS root and mounts for most use cases in Unity Catalog-enabled workspaces because they lack these critical governance features, making volumes the go-to solution for production workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0459ee32-a173-47fc-a20c-2afb3664cc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Data files to use in this usecase:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f5b44c3-2de0-4897-9fcc-d09c4155e739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders: Customer → /Volumes/.../customer/ Usage → /Volumes/.../usage/ Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2497989c-8391-459f-aad9-353e8f816773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#customer_csv = ''' 101,Arun,31,Chennai,PREPAID 102,Meera,45,Bangalore,POSTPAID 103,Irfan,29,Hyderabad,PREPAID 104,Raj,52,Mumbai,POSTPAID 105,,27,Delhi,PREPAID 106,Sneha,abc,Pune,PREPAID '''\n",
    "\n",
    "customer_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.txt\"\n",
    "\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv\",customer_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb9554e-596d-491d-983f-02dce5fafab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(dbutils.fs.head(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7fde010-938a-4422-b16c-abf483e49cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\\n101\\t320\\t1500\\t20\\n102\\t120\\t4000\\t5\\n103\\t540\\t600\\t52\\n104\\t45\\t200\\t2\\n105\\t0\\t0\\t0 '''\n",
    "usage_tsv = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.txt\"\n",
    "\n",
    "volume_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_csv\"\n",
    "\n",
    "dbutils.fs.put(volume_path, usage_tsv,overwrite=True)\n",
    "print(dbutils.fs.head(volume_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c14bec-baf6-4ddf-acb0-4a1ae1af66d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp 5001|101|TWR01|-80|2025-01-10 10:21:54 5004|104|TWR05|-75|2025-01-10 11:01:12 '''\n",
    "\n",
    "volume_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1\"\n",
    "\n",
    "dbutils.fs.put(volume_path, tower_logs_region1,overwrite=True)\n",
    "print(dbutils.fs.head(volume_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce588d6-ff05-406e-b70f-ec02d00f3708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5576179c-59b2-4946-99ab-66d79c1eaa11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a216fa93-e7da-475d-9c97-76277d8eca1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f524883-c941-4f29-a977-0ebcdbf447fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a83fd00-ac66-44ea-b47b-26b05844ae22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Directory Read Use Cases\n",
    "1. Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131aec71-d001-4ab8-8d33-0fb51a221221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"\n",
    "\n",
    "#Read the csv files using pathGlobFilter\n",
    "tower_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"pathGlobFilter\",\"*.csv\").load(file_path)\n",
    "\n",
    "display(tower_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a5c7ec-22b0-430d-a757-393018ded62c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/\"\n",
    "\n",
    "recursive_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").load(file_path)\n",
    "\n",
    "display(recursive_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56b72412-99f5-456d-ba89-285c793387b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Schema Inference, Header, and Separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252da4e2-13c8-49f6-901b-034b5c5215b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:\n",
    "header=false, inferSchema=false\n",
    "or\n",
    "header=true, inferSchema=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2daff93f-0909-42a7-a744-9fefeb612f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.txt\", header=True, inferSchema=False)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e249faa-2d4a-4fbc-8c9f-39b364958f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.txt\", header=False, inferSchema=True)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5f7ed9-2d71-413b-ad06-d112827ff04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5. Column Renaming Usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8235d9b-ad5e-41b8-9efb-241ec978c5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Apply column names using string using toDF function for customer data\n",
    "df3 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.txt\").toDF(\"id\",\"name\",\"age\",\"location\",\"plan\")\n",
    "\n",
    "df3.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41767a99-c68b-4474-9402-489adaccd2d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Apply column names and datatype using the schema function for usage data\n",
    "df4 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.txt\",sep=\"\\t\",header=True,inferSchema=False).toDF(\"cust_id\",\"voice_mins\",\"data_mb\",\"sms_count\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732a2478-7626-4d9c-aa22-f9751ed95383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"tower_id\", StringType(), True),\n",
    "    StructField(\"signal_strength\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "tower_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region3.csv\", schema=custom_schema)\n",
    "\n",
    "tower_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2278dc94-59a9-48a1-9c83-119d396aef11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Spark Write Operations using\n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed220984-43e5-4260-a801-6695a43983d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b53dc130-63ad-4ed3-9426-6c50a1b60310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into CSV format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd9f4a8-a3cc-4375-b2fc-45a30e0fbef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/csvout\",header=True,sep=\",\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ddcf05-daad-4cb1-8bd2-fedfca3c9b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into CSV format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b9a060-0c63-4f7a-acb2-8dbf73266141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usgout\",header=True,sep=\",\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8693d673-e4e4-4904-bddc-85c497410b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Write tower data into CSV format with header enabled and custom separator (|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937bc207-5aaf-4dc0-8c5c-9e5182df47b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#need to workout tower data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9cbae7a-abec-4258-8c91-f023bf87bb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read the tower data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f193dc7f-f61a-4d83-89c8-36725633ca49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157fbb58-240e-4588-a9a1-76b43b2a88c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7812c1-7821-4b52-a68b-5e8ef1b96836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3fb4820-12f8-4338-8628-e286f449f560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into JSON format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b3bfa8-b520-4aa1-a221-6a8322970d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsonout\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f34f4cc-6de1-484a-9c85-be0b5ce21548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into JSON format using append mode and snappy compression format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd362d46-9888-4fdf-8754-1a50bf20c40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/jsonout\",mode=\"append\",compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6776430-0253-4974-8442-c78b0f148082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8417356d-e699-45e5-817c-e9a23c8fe6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#need to work out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "945d4acc-8e65-49e8-b89a-9bc7b8d357e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read the tower data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b03870-6600-4e27-91b7-2cbc6c056e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f6a64a8-bf0f-47ea-bada-c390421255ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0816fcd4-1540-4e49-957f-faf6cfb00ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0956b0-d82e-47f7-af29-297ed6019fa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf1d7aa-efa0-4f06-9224-f7aec5b65aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f536b9-e0bd-4428-9199-911e8e570943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout\",mode=\"overwrite\",compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "769a997f-6f8c-41ad-b79c-350f02faeecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into Parquet format using error mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f733e7-e0c5-4f34-88d1-14d56f64c12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout\",mode=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb46549-3bc6-4024-8cd8-0f14e854d0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Write tower data into Parquet format with gzip compression option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abdc635c-e9cc-458d-b62a-8b94aba43529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Need to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69836631-3ea0-4b2e-9325-5c532afda8ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read the usage data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81ce170-2ddb-4488-a40c-d98910e6cb03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/parquetout\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13ccd270-e841-4258-a026-a714af366151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b4673c-3255-49f3-bff8-b69a81c1a3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "831ba7ad-bff0-4715-b922-bcd97d3c69bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13821b78-5e10-40eb-a00a-201e789ee60d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into ORC format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48fc96d-1cdd-4c5a-abc8-4398cf766f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout\",mode=\"overwrite\",compression=\"zlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38807c3c-a10c-477d-a405-209e034c89bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into ORC format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276358fd-6d47-4d38-ad8c-fdbbb4c2529b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d353534-072b-4b2a-8fb2-426cff4272ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Write tower data into ORC format and see the output file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8592aaf8-2241-4e71-a6d2-1dd2c04cee66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Need to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6041ecb-206e-475a-b5c9-d6c09412792f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read the usage data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2933b1f-b73c-47a7-b4e2-ab11017c3362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_orc_df = spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/orcout\")\n",
    "usage_orc_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c86b08f-ec48-4931-a43f-b4bd77e6d1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2f4445-854f-4b34-a9e8-bbf188717a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "899c3a58-7df5-4555-aee6-ef7867e819a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "457e2c6f-5dbb-45c4-90ba-a511a538746b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into Delta format using overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0574071e-6374-47b6-9c89-45fd93a476d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltaout\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46809c58-aa7c-4ca9-9ac5-5514fe92b740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into Delta format using append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41eb0c23-a33e-47d7-b83f-a73861a49aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e19900d-7520-4d40-8ff3-8d08ac851b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Write tower data into Delta format and see the output file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bead471-75fb-4c8d-90de-fbadfc44e456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Need to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d609a43-dcf9-429d-a5bf-ed707a2d12c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read the usage data in a dataframe and show only 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793e7475-5b33-4bc7-87e5-e622824b9ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_deltadf = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/deltaout\")\n",
    "usage_deltadf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dba6b30-0fad-480a-b93f-02b2a520a76b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df6f1c7-c5c0-4a69-807f-d42336bdb95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9f12cd-7307-453e-b7a3-c901e134acf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b21cd1-08ea-442a-84d2-414373c3e3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Parquet location is having started, commited and part file, whereas Delta location is having only Part file along with delta_log file. _delta_log file contains the metadata and _staged_commits. Whereas Parquet is having success and commited files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc91a91d-20f6-4483-8bdb-7caebfcb4b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###11. Write Operations (Lakehouse Usecases) – Delta table Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a157f5b8-843b-4383-a8d5-be8c15efb48a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data using saveAsTable() as a managed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f24d6eab-b11e-4687-99c1-80102192d2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.saveAsTable(\"workspace.wd36schema.lh_custtbl\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb218972-f770-48a6-8331-dd62beaa0350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data using saveAsTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27916763-1450-4c0a-8751-d32df63bcf40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.saveAsTable(\"workspace.wd36schema.lh_usgtbl\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b7b22b6-98f4-42e7-8a5d-e0fd456ab1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Drop the managed table and verify data removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4027be-b7e4-4cab-8ff8-b842a766bf9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS workspace.wd36schema.lh_custtbl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f51410b8-75e8-414f-9d39-5f5799c47c7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "\n",
    "Not sure what the question is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8298da26-b6af-4d3e-8cad-37674484c6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Use spark.read.sql to write some simple queries on the above tables created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b999a06-7eee-4839-a20f-cf87e4e010c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from workspace.wd36schema.lh_usgtbl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c36d7f85-5b8a-4a10-9a63-cbb246bbcd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###12. Write Operations (Lakehouse Usecases) – Delta table Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fa52e3-755a-4fe1-9030-e52ef3a3feec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data using insertInto() in a new table and find the behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a9cfa4-aad1-4fb4-b1f1-0deb7a4dccba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.insertInto(\"workspace.wd36schema.lh_custtbl\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9986ef6-db74-47a8-b4bc-796efb8a5c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Above statement shows insertinto() needs table to be created prior to executing the statement.New table cannot be created by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33177eb1-8f76-4cc2-819a-c3592ce3dde7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18ba271-0c5d-40ba-9cc4-de120fe4f081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.insertInto(\"workspace.wd36schema.lh_usgtbl\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ea04fe-8e33-43f4-a2f2-ee20ae3005e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 13. Write Operations (Lakehouse Usecases) – Delta table Usecases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69a9151d-6525-443a-9c7f-c3e23a550822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Write customer data into XML format using rowTag as cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e96180a-bc58-4b53-a66d-7443c3a54025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xmlout\",mode='overwrite',rowTag=\"cust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cebb00b-57bb-44d5-8e0f-c3f15a75dd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc39c8b-cb88-46c0-a62d-2bb1195c78d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/xmlout\",rowTag=\"usage\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeeaf11e-2f7e-419d-94e4-ab240b0f9089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cf6fa81-13f2-455e-a8d7-48645acff950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2108e36e-77b1-46a8-996e-f625ff96e865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e3e8c73-5126-4967-b9cc-ddd1ef5532ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3fc9d9-1203-4625-b4d3-905a5fe8e472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "csv = 106 bytes\n",
    "json = 344 bytes\n",
    "ORC = 685 bytes\n",
    "parquet = 1.25 kb\n",
    "delta = 1.25 kb\n",
    "xml = 874 bytes\n",
    "\n",
    "Ordered based on size\n",
    "csv < json < orc < xml < parquet < delta\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7e0dee-8272-4827-a33d-407f21365831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a0351c-d0dd-4303-b47e-1ba6998bd841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16243c28-a83e-4222-9b17-b3e757a41a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orcdf = spark.read.orc(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/orcout/part-00000-tid-75013927119242283-7cf32933-e9e9-47f7-bc03-f603dac0dfcc-218-1-c000.zlib.orc\")\n",
    "orcdf.write.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquettestout\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68986ceb-46ee-494f-ae2d-2af4a7f48409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eef00b7-6f32-4950-bd2e-e3536bc2debc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pardf = spark.read.parquet(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/parquetout/part-00000-tid-4123262550380396419-e63f8747-44d4-4ef4-8940-b2751ef98d11-286-1-c000.gz.parquet\")\n",
    "pardf.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltatestout\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4b65cf6-fb79-438d-b2b0-a3348168e072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941ee47d-42eb-485d-8f8b-3ce8c727e2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltadf = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/deltatestout\")\n",
    "deltadf.write.xml(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/xmltestout\",rowTag=\"cust_id\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa68d52-3649-4a7b-8e0a-d90944881c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- when we want to read a single Delta file using the Delta format, but Delta expects a directory containing the Delta transaction log and data files, not an individual Parquet file. To fix this, load the entire Delta table directory instead of a single file.\n",
    "- when we write in xml format rowTag is mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a53bea3a-36b4-4a8e-aa8a-eba2ccfb8b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323c629d-5eb1-49a4-aa7c-5f704af98621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltadf.write.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/jsontestout\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3060853-b3fa-4a12-85c5-4865f22b9e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6ec555-3468-48e4-b944-a1523771e46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltadf.write.saveAsTable('workspace.wd36schema.lh_newtbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae22aa3d-ef58-4f2d-a30c-5661eddeaf9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 16. Do a final exercise of defining one/two liner of..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6155694-7bcd-400f-ad9a-78d11a8c453b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbbbad28-ddef-4875-8025-640524ab30e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8282819638152786,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecases.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
